\documentclass[12pt,a4paper]{report}

\usepackage{styles/dolgozat}

\usepackage{listings}
\usepackage{styles/cpp}
\usepackage{styles/python}
\usepackage{pdfpages}

\usepackage{hyperref}

\begin{document}

\include{cover/cimlap}

\newpage

\pagestyle{empty}

\includepdf[pages=-]{cover/diplomamunka_kiiras.pdf}

\include{cover/feladatkiiras}

\cleardoublepage
\pagenumbering{gobble}
\tableofcontents
\cleardoublepage
\pagenumbering{arabic}

\newpage

\pagestyle{fancy}

\include{chapters/1_introduction}
\include{chapters/2_nlp}
\include{chapters/3_modern_solutions_in_nlp}
\include{chapters/4_implementation}
\include{chapters/5_test}
\include{chapters/6_summary}

\addcontentsline{toc}{chapter}{Irodalomjegyzék}
\bibliographystyle{unsrt}
\begin{thebibliography}{100}
\bibitem{history} Johri, Prashant \& Khatri, Sunil Kumar \& Al-Taani, Ahmad \& Sabharwal, Munish \& Suvanov, Shakhzod \& Chauhan, Avneesh. (2021). Natural Language Processing: History, Evolution, Application, and Future Work. 10.1007/978-981-15-9712-1\_31.

\bibitem{enigma} Wikipedia: Military Enigma machine, model "Enigma I" - \url{https://en.wikipedia.org/wiki/Enigma_machine#/media/File:Enigma_(crittografia)_-_Museo_scienza_e_tecnologia_Milano.jpg}

\bibitem{shrdlu} hci.stanford.edu: SHRDLU - \url{https://hci.stanford.edu/~winograd/shrdlu/}

\bibitem{ibm_trans} John Hutchins. (2006). The first public demonstration of machine translation : the Georgetown-IBM system , 7 th January 1954

\bibitem{chomsky} Mounin, Georges. (1961). Chomsky, Noam: Syntactic Structures. Babel. 7.10.1075/babel.7.1.13mou.

\bibitem{atn} Woods, William A. (1970). Transition Network Grammars for Natural Language Analysis

\bibitem{questions} Bánki Dezső \& Bognár László \& Garai Zsolt \& Forrai Gábor \& Margitay Tihamér \& Máté András \& Mekis Péter \& Tanács János \& Zemplén Gábor. (2006). Esszéírás és informális logika

\bibitem{question_generation} Le, NT., Kojiri, T., Pinkwart, N. (2014). Automatic Question Generation for Educational Applications – The State of Art.

\bibitem{autoquest} John Harmon Wolfe. (1976). Automatic question generation from text - an aid to independent study.

\bibitem{reading_tutor} Chen, W., Aist, G., \& Mostow, J. (2009). Generating Questions Automatically from Informational Text

\bibitem{wiki_qg} Jouault, C. and Seta, Kazuhisa. (2013). Building a semantic open learning space with adaptive question generation support

\bibitem{human_reading} Hulme C, Snowling MJ. Learning to Read: What We Know and What We Need to Understand Better. Child Dev Perspect. 2015 Mar 1;7(1):1-5. doi: 10.1111/cdep.12005. PMID: 26290678; PMCID: PMC4538787.

\bibitem{rnn} Wikimedia: Recurrent neural network unfold - \url{https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg}

\bibitem{attention} Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Lukasz and Polosukhin, Illia. (2017). Attention Is All You Need.

\bibitem{translation} Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Y.. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. ArXiv. 1409.

\bibitem{bert} Tassopoulou, Vasiliki. (2019). An Exploration of Deep Learning Architectures for Handwritten Text Recognition. 10.13140/RG.2.2.34041.62565.

\bibitem{bert2} Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

\bibitem{t5} ai.googleblog.com: Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer -  \url{https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html} (2020.02.24)

\bibitem{gpt} Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei. (2020). Language Models are Few-Shot Learners

\bibitem{in_context_learning} Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?

\bibitem{hf} huggingface.co: Hugging Face Documentation - https://huggingface.co/docs/

\bibitem{squad} Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text
\end{thebibliography}


\noindent \textit{Az internetes források utolsó ellenőrzése: 2023.05.07}

\pagestyle{empty}

\newpage

\include{cover/hasznalati}

\end{document}
