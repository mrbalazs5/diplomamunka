\documentclass[12pt,a4paper]{report}

\usepackage{styles/dolgozat}

\usepackage{listings}
\usepackage{styles/cpp}
\usepackage{styles/python}

\usepackage{hyperref}

\begin{document}

\include{cover/cimlap}

\newpage

\pagestyle{empty}

\include{cover/feladatkiiras}

\cleardoublepage
\pagenumbering{gobble}
\tableofcontents
\cleardoublepage
\pagenumbering{arabic}

\newpage

\pagestyle{fancy}

\include{chapters/1_introduction}
\include{chapters/2_nlp}
\include{chapters/3_modern_solutions_in_nlp}
\include{chapters/4_implementation}
\include{chapters/5_test}
\include{chapters/6_summary}

\addcontentsline{toc}{chapter}{Irodalomjegyzék}
\bibliographystyle{unsrt}
\begin{thebibliography}{100}
\bibitem{history} Johri, Prashant \& Khatri, Sunil Kumar \& Al-Taani, Ahmad \& Sabharwal, Munish \& Suvanov, Shakhzod \& Chauhan, Avneesh. (2021). Natural Language Processing: History, Evolution, Application, and Future Work. 10.1007/978-981-15-9712-1\_31.

\bibitem{ibm_trans} John Hutchins. (2006). The first public demonstration of machine translation : the Georgetown-IBM system , 7 th January 1954

\bibitem{chomsky} Mounin, Georges. (1961). Chomsky, Noam: Syntactic Structures. Babel. 7.10.1075/babel.7.1.13mou.

\bibitem{question_generation} Le, NT., Kojiri, T., Pinkwart, N. (2014). Automatic Question Generation for Educational Applications – The State of Art.

\bibitem{rnn} Recurrent neural network unfold, \url{https://commons.wikimedia.org/wiki/File:Recurrent\_neural\_network\_unfold.svg}

\bibitem{attention} Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Lukasz and Polosukhin, Illia. (2017). Attention Is All You Need.

\bibitem{translation} Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Y.. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. ArXiv. 1409.

\bibitem{bert} Tassopoulou, Vasiliki. (2019). An Exploration of Deep Learning Architectures for Handwritten Text Recognition. 10.13140/RG.2.2.34041.62565.

\bibitem{bert2} Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

\bibitem{t5} Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html

\bibitem{gpt} Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei. (2020). Language Models are Few-Shot Learners

\bibitem{in_context_learning} Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?

\bibitem{hf} Hugging Face Documentations https://huggingface.co/docs/

\bibitem{squad} Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang. (2016). SQuAD: 100,000+ Questions for Machine Comprehension of Text
\end{thebibliography}


\noindent \textit{Az internetes források utolsó ellenőrzése: 2023.04.23}

\pagestyle{empty}

\newpage

\include{cover/hasznalati}

\end{document}
