\Chapter{Összegzés}

Mint láthattuk dolgozatomban is, a természetes nyelvfeldolgozás virágkorát éli napjainkban. Egymás után jelennek meg olyan alkalmazások, chatbotok melyek szinte tökéletesen képesek megérteni az emberi nyelveket, képesek szöveges feladatokat megoldani, kódokat generálni, cikkeket értelmezni és írni, szöveges hibákat keresni vagy ahogy láthattuk is immáron képesek értelmes kérdéseket is generálni nagyon jó eredményekkel. 

Úgy gondolom, hogy alkalmazásunk remekül beilleszthető ezen megoldások mellé, de nyilván van rajta még javítani való. Láthattuk, hogy a ChatGPT-vel összehasonlítva azért teljesítményben van még hova fejlődnie, de kisebb mérete és jobb hardvereken való gyorsabb teljesítménye, így is számos plusz pontot ad programunknak.

Ahol lehetne még fejleszteni az alkalmazást az mindenképp a tudásbázisa. Amellett, hogy többnyelvűvé is lehetne tenni, növelni lehetne a tudását egy jobban szűrt, többféle kérdés és kontextustípust is tartalmazó tanítóhalmazzal. Továbbá nem ártana a hardveres erőforrásokon is javítani és egy még nagyobb, GPU-t vagy TPU-t kihasználó felhőszolgáltatáson taníttatni és működtetni, ahol kihozhatnánk belőle a maximumot. Ez természetesen a többi neurális hálózatokat használó alkalmazás problémája is, hiszen a hálózat méretét növelve egyre jelentősebb lesz a teljesítményigény is, így többnyire nagyobb cégek tudnak ezekhez megfelelő erőforrásokat biztosítani.

Ehhez kapcsolódóan fontos kérdés lehet még ezen mély tanulás alapú megoldások, illetve magának az informatikai szektornak a teljesítményigénye, hiszen egy-egy komolyabb modell tanítása óriási költségekkel járhat, ami a hibajavítást is megnehezítheti, hiszen nagyobb volumenű hibáknál sokszor nincs lehetőség az egész hálózatot újratanítani, mivel az akár hónapokig is eltarthat és rengeteg erőforrást felemészthet. Természetesen a technológia fejlődése itt is megoldhatja a problémákat, de a jövőben erre mindenképp oda kell majd figyelni.

Végül fontosnak tartom még megemlíteni az ilyen neurális hálózat alapú, NLP megoldások társadalmunkat is érintő következményeit. Világosan látszik, hogy közelítünk egy olyan ponthoz, ahol a 20. században felvázolt gépi intelligenciát behatároló korlátokat kezdjük átlépni. Sorra jelennek meg olyan alkalmazások a terület kutatásai alapján, melyek képesek megoldani olyan feladatokat, amiket eddig csak emberek, külön képesítéssel tudtak elvégezni, azonban egyre többször találkozhatunk azzal is, hogy a gép szintaktikailag jól oldotta meg a feladatot, de valójában helytelen eredményeket generált. Úgy gondolom ezen a területen is rengeteget kell még fejlődnie a tudománynak és a neurális hálózatokat önmagyarázóvá és könnyebben karbantarthatóvá kell tenni, más különben eláraszthatják az internetet hamis, pontatlan információkkal, amik egy láncreakció folytán a később modellek eredményeit is ronthatják, illetve társadalmunkra is negatív hatással lehetnek.

Úgy vélem dolgozatom jó táptalaj lehet ezen megoldások elkészítéséhez a jövőben és én is törekedni fogok a terület népszerűsítésére és kutatására az elkövetkezendőkben.

\Chapter{Summary}

As we saw in my thesis, natural language processing is currently in its heyday. Applications and chatbots appear one after another that are able to understand human languages almost perfectly, are able to solve text tasks, generate codes, interpret and write articles, search for text errors or, as we have seen, are now able to generate meaningful questions with very good results.

I think that our application can be perfectly integrated with these solutions, but there is obviously room for improvement. We could see that, compared to ChatGPT, it still has room for improvement in terms of performance, but its smaller size and faster performance on better hardware still give our program many plus points.

Where the application could still be improved is definitely its knowledge base. In addition to making it multilingual, it's knowledge could be increased with a more filtered training set containing several types of questions and contexts. Furthermore, it would not hurt to improve the hardware resources and have it taught and operated on an even larger cloud service that uses GPU or TPU, where we could get the most out of it. This is, of course, also the problem of other applications using neural networks, since increasing the size of the network will also increase the performance requirements, so mostly larger companies can provide adequate resources for these.

Relatedly, these deep learning-based solutions and the performance requirements of the IT sector itself can also be an important issue, since training a serious model can entail enormous costs, which can also make error correction difficult, since it is often not possible to retrain the entire network in the case of large-volume errors, since it can take months and consume a lot of resources. Of course, the development of technology can solve the problems here as well, but in the future we will definitely have to pay attention to this.

Finally, I consider it important to mention the consequences of such neural network-based NLP solutions that also affect our society. It is clear that we are approaching a point where we are starting to cross the boundaries of machine intelligence outlined in the 20th century. Applications based on research in the field are appearing one after another, which are able to solve tasks that until now only humans, with special qualifications, could do, but we can also come across more and more that the machine solved the task syntactically well, but actually generated incorrect results. I think that science still needs to improve a lot in this area and neural networks need to be made self-explanatory and easier to maintain, otherwise they can flood the internet with false and inaccurate information, which, due to a chain reaction, can worsen the results of later models and have a negative impact on our society. .

I believe that my thesis can be a good breeding ground for preparing these solutions in the future, and I will also strive to popularize and research the field in the future.